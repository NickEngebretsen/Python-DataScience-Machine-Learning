{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "## ECE 204 Data Science & Engineering\n",
    "\n",
    "*Warning: this notebook reuses certain variable names, which can cause conflict. They're found in multiple questions. If you have any difficulty, please restart your kernel in the menu above.*\n",
    "\n",
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are few questions that deal with the Wisconsin Breast Cancer dataset `\"breast-cancer-wisconsin.csv\"`. The fun fact about this data is that it originates from the University of Wisconsin itself! If you are interested, you can read more about it here: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) <br>\n",
    "\n",
    "The 30 features in this data describe certain characteristics of a cell nuclei that can be used to predict whether a tumor is `Malignant` or `Benign`. We will employ the KNN algorithm to do this classification/diagnosis.\n",
    "\n",
    "The `diagnosis` column in this data contains the \"label\" information for training where the labels `0 = \"Benign\"` and `1 = \"Malignant\"`. The other columns represent the various features. You can find detailed attribute information regarding what each feature means at the link above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 1.**\n",
    "We have 30 features in this dataset. As it often turns out, some features are more important than others in a given scenario. Our ultimate objective is to classify whether a tumor is malignant or benign based on a certain set of features in a data point. This data is encoded in the `diagnosis` column as 1 and 0 respectively.\n",
    "\n",
    "Plot the following feature columns against each other. **Which pair of features represent the most clear separation of classes among the pairs given below?**\n",
    "\n",
    "1. area_mean vs concavity_mean\n",
    "2. fractal_dimension_mean vs symmetry_mean\n",
    "3. smoothness_mean vs texture_mean\n",
    "4. texture_mean vs fractal_dimension_mean\n",
    "\n",
    "`Hint:` While plotting, remember that you can use `dfbc.plot.scatter(..., c=column_name, cmap='viridis')`. This will show `column_name` in different colors based on value. Using different colors for different classes will help you visualize the separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code given for convenience\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reading data\n",
    "dfbc = pd.read_csv(\"breast-cancer-wisconsin.csv\")\n",
    "dfbc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 2.**\n",
    "In the Wisconsin-Breast-Cancer dataset from last question, train a KNN model with $K = 11$ on provided preprocessed dataset described below. **What is the accuracy of this model?**\n",
    "\n",
    "As a preprocessing step, you have been given code that does the following:\n",
    "\n",
    "- splits the dataset into a train/test set. It uses 70% of the dataset for training, and 30% for testing.\n",
    "- normalizes the features. It scales each feature so the mean is 0 and the standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code given for convenience\n",
    "# First, a bunch of imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reading data\n",
    "dfbc = pd.read_csv(\"breast-cancer-wisconsin.csv\")\n",
    "\n",
    "# Defining features and labels\n",
    "Y = dfbc.iloc[:, 0]  # the first column is the label\n",
    "X = dfbc.iloc[:, 1:]  # the other columns are the features\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Normalize the dataset\n",
    "sc = StandardScaler()\n",
    "X_train_normalized = sc.fit_transform(X_train)\n",
    "X_test_normalized = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 3.**\n",
    "Let's try to find the best value of K for the example in the previous question. You have already been given preprocessing steps for splitting and normalizing the data. Fit the KNN on the normalized training, and then measure the accuracy on the normalized testing data. Do this for all values of K from 1 to 20. **What value of K in the range 1-20 yields the highest accuracy score for the normalized test set?**\n",
    "\n",
    "*Hint:* Run the classifier model fitting and predicting in a loop for each value of K from 1 to 20, saving the value of accuracy at each iteration in a list or a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code given for convenience (same as previous question)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reading data\n",
    "dfbc = pd.read_csv(\"breast-cancer-wisconsin.csv\")\n",
    "\n",
    "# Splitting the dataset\n",
    "X = dfbc.iloc[:, 1:]\n",
    "Y = dfbc.iloc[:, 0]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Normalize the dataset\n",
    "sc = StandardScaler()\n",
    "X_train_normalized = sc.fit_transform(X_train)\n",
    "X_test_normalized = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of two questions will focus on the data in `airports.csv`. The dataset lists over 3,000 airports in the US, including the state they're located in, the latitude, and the longitude. These questions will ask you to train a decision tree classifier from string labels in the prediction variables. Scikit-Learn supports using string arrays as prediction variables, at least when doing classification.\n",
    "\n",
    "---\n",
    "**Problem 4.**\n",
    "Train a decision tree classifier on the `airports.csv` dataset to predict the state from the latitude and longitude of each airport. **What's the importance of `latitude` as reported by the Scikit-Learn decision tree object?** If the feature importance for `latitude` is high, state boundaries tend to run east-west instead of north-south.\n",
    "\n",
    "When creating the decision tree classifer, use `random_state=0` and `max_depth=4`. \n",
    "Round your answer to 2 digits (e.g., 0.29 for 0.29132). \n",
    "\n",
    "*Hint:* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"airports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 5.**\n",
    "Using the data in `airports.csv`, train a decision tree to predict the state from the `latitude` and `longitude`. When creating the decision tree classifier, use `max_depth=4` and `random_state=0`.\n",
    "\n",
    "**What state does the decision predict for the point  predict for** [latitude, longitude] = [43.2397, -75.1578]? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"airports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 6.**\n",
    "Read in `banknote_train.csv` and `banknote_test.csv` datasets. The data has 4 feature columns, and one target/label column. The target column is \"1\" if the banknote is forged, and \"0\" if it's authentic.\n",
    "\n",
    "Build a decision tree classifier with `random_state=0` and `max_depth=1`. This is the simplest estimator; it's only making a single binary chioce. Using this estimator, predict the labels for the data in test file after training on the data in the train file.\n",
    "\n",
    "**What is the accuracy of the trained decision classifier on the test data?** \n",
    "\n",
    "`Note`: In Canvas, enter your answer as a percent between 0 and 100, rounded to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"banknote_train.csv\")\n",
    "df_test = pd.read_csv(\"banknote_test.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Problem 7.**\n",
    "**The number of examples given to an estimator affects how well it performs.** To investigate this, let's increase the number of examples (or rows in the dataframe) the estimator gets for training, and evaluate accuracy on a test set for each estimator.\n",
    "\n",
    "Read the `banknote_train.csv` and `banknote_test.csv` datasets. Train 4 different decision tree classifiers with the following training dataset:\n",
    "\n",
    "1. Training dataset: up to row index 200, inclusive\n",
    "2. Training dataset: up to row index 400, inclusive\n",
    "3. Training dataset: up to row index 900, inclusive\n",
    "4. Training dataset: up to row index 1100, inclusive\n",
    "\n",
    "These datasets are given in some example code.\n",
    "\n",
    "**Compute the accuracy on testing data from these 4 models. In Canvas, select all the true statements.** Specify `random_state=0` when creating the decision tree classifier, and let `max_depth` be the default value (meaning you don't need to specify it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"banknote_train.csv\")\n",
    "df_train1 = df_train.iloc[0:200 + 1]\n",
    "df_train2 = df_train.iloc[0:400 + 1]\n",
    "df_train3 = df_train.iloc[0:900 + 1]\n",
    "df_train4 = df_train.iloc[0:1100 + 1]\n",
    "\n",
    "df_test = pd.read_csv(\"banknote_test.csv\")\n",
    "test_total = len(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
