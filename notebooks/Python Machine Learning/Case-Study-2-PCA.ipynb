{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis on company data**\n",
    "## ECE 204 Data Science & Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and NumPy to deal with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the required module from sklearn to perform PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset `companies.csv` and analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"companies.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize both of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use column INDEX (0 and 1) instead of names (\"employees\" and \"revenue_usd\") because it's shorter to type!\n",
    "ax = df.plot.scatter(x=0,y=1, title=\"Company Data\")\n",
    "\n",
    "# What about the scale?\n",
    "#ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these data that the revenue of a company is strongly related to the number of employees the company has. <br> Revenue tends to increase as the number of employees increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module from sklearn for data standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler is used for data standardization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# We define a StandardScaler and then we fit it to our data\n",
    "scaler.fit(df)\n",
    "\n",
    "# After running the fit method, the normalize object will have the attributes mean_ (the mean) and scale_ (the standard deviation)\n",
    "print(\"Mean of the data is:\", scaler.mean_)\n",
    "print(\"Standard Deviation of the data is:\", scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can standardize the data using the transform method.\n",
    "numpy_scaled = scaler.transform(df)\n",
    "\n",
    "# .transform returns a NumPy array, which we then convert into a Pandas DataFrame.\n",
    "df_scaled = pd.DataFrame(numpy_scaled, columns=[\"employees_scaled\",\"revenue_usd_scaled\"])\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New mean and standard deviation\n",
    "print(\"Mean of the data is:\", df_scaled.mean().values.round(2))\n",
    "print(\"Standard Deviation of the data is:\", df_scaled.std().values.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_scaled.plot.scatter(x=0, y=1, title=\"Normalized Data\");\n",
    "#ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Let's run `PCA` to reduce this 2D dataset to a 1D dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1) # reduce it to 1D (this is all we can really do, since the original data is only 2D)\n",
    "pca.fit(df_scaled);\n",
    "data_pca = pca.transform(df_scaled)\n",
    "\n",
    "print(\"The original data has shape\", df.shape )\n",
    "print(\"The transformed data has shape\", data_pca.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take our reduced data and perform the inverse transformation to see what was lost in the reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inv = pca.inverse_transform(data_pca)\n",
    "print(\"The inverse transformed data has shape\", data_inv.shape)\n",
    "\n",
    "# Convert the inverse transformed data into a dataframe\n",
    "df_scaled_inv = pd.DataFrame(data_inv, columns=df_scaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now visualize the original and the PCA projected data to see how well PCA performed\n",
    "\n",
    "**Aside:** Here we plot two plots on top of each other. This is done by making the axes of the two plots same. We get the axis of the first plot `ax1` and make it same as the axis of the second plot using `ax=ax1`. \n",
    "\n",
    "**Aside:** The `alpha` argument makes the plot transparent. We can see that the first plot is lighter and the second is much darker as we have passed `alpha=.2` for the first plot and `alpha=1` for the second plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = df_scaled.plot.scatter(x=0, y=1, alpha=.2, color='r', label=\"Original Data (norm)\")\n",
    "df_scaled_inv.plot.scatter(x=0, y=1, alpha=1, ax=ax1, label=\"PCA Projected Data (norm)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to return to the original coordinates, use the \"inverse transform\" from our standardization\n",
    "df_inv = pd.DataFrame(scaler.inverse_transform(df_scaled_inv), columns=df.columns) \n",
    "\n",
    "ax2 = df.plot.scatter(x=0, y=1, alpha=.2, color='r', label=\"Original Data\")\n",
    "df_inv.plot.scatter(x=0, y=1, alpha=1, ax=ax2, label=\"PCA Projected Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above plot, PCA \"flattened\" the 2D dataset into a 1D dataset. The information along the least important principal axis is removed, leaving only the component of the data with the highest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much of the variance is explained by using just one component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((np.random.randn(100000)*1.2)>0.67).sum()/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
