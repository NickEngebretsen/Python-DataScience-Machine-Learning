{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PCA with MNIST**\n",
    "## ECE204 Data Science & Engineering\n",
    "\n",
    "Let's investigate a real world example of PCA with the [MNIST digits dataset][mnist]. This is a very popular dataset that looks like:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "[mnist]:https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "This is a very simple dataset. **How good is PCA at finding the components that best describe this dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.load(\"mnist.npz\")\n",
    "X = data[\"X\"]\n",
    "labels = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is 70,000 images, each of which are 28x28, or have 784 pixels each. The rows are concatenated to conform with \"tidy\" data, which is what Scikit-Learn expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are 28 x 28 images, which means that there are 784 pixel values for each image. Each row of X has 784 numbers (which is 28x28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one of the digits (reshape to 28x28)\n",
    "np.set_printoptions(linewidth=300)\n",
    "X[5].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's better visulize this as an image. To do this, matplotlib will need to be imported (Pandas plotting is a wrapper around matplotlib):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[5].reshape(28,28), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a random selection of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a d1 x d2 grid of subplots\n",
    "rows, cols = 4, 6\n",
    "fig, axs = plt.subplots(nrows=rows, ncols=cols, figsize=(8,6))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "    \n",
    "        # pick an image at random\n",
    "        k = np.random.randint(len(labels))\n",
    "\n",
    "        # reshape the image\n",
    "        img = X[k].reshape(28,28)\n",
    "        label = labels[k]\n",
    "\n",
    "        # plot the image\n",
    "        axs[i,j].imshow(img, cmap='gray_r')\n",
    "        axs[i,j].set_title(str(label))\n",
    "        axs[i,j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this cell multiple times will show different images because no random seed is set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is complicated because the handwritten numbers...\n",
    "\n",
    "* are in different places\n",
    "* are of different widths\n",
    "* have different shapes\n",
    "\n",
    "This prompts a couple questions:\n",
    "\n",
    "1. How many dimensions are required to effectively code this data?\n",
    "2. How good is that embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many dimensions are needed?\n",
    "\n",
    "Specifically, how many dimensions are needed to explain 90% of the variance? This will look at the standard deviation of the same pixel in different images. However, there's no control on image location: the handwriting can appear anywhere. **Why should the standard deviation of the same pixel in different images mean anything?**\n",
    "\n",
    "This can be found by passing computing all the principal components, then using the `explained_variance_ratio_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "embedding = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *could* say \"explain 90% of the variance\", but let's look at *all* data PCA has to offer and make sure that 0.95 is a good value.\n",
    "\n",
    "The [Scikit-Learn PCA documentation][pca] explains that when `PCA()` is called with no arguments, it computes *all* the principal components.\n",
    "\n",
    "[pca]:https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit embedding.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dims = pd.Index(range(1,785), name=\"dimensions\")\n",
    "dim_explain = pd.Series(embedding.explained_variance_ratio_, index=dims)\n",
    "dim_explain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information says \"the 4th principal component explains 5.4% of the variance\", and so on. To see the cumulative effect (variace explained by the first k components combined), where k=1,2,..., we can use the Series `cumsum` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_explain.cumsum().plot(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like around 100 features are needed in the MNIST dataset to explain 90% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another method to find how many principal components explain 90% of the variance:\n",
    "\n",
    "> If called with just a number between 0 and 1, it uses just enough principal components to explain this amount of variance. More information about this in the documentation:\n",
    ">\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "Let's use that to explain 90% of the variance for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = PCA(n_components = 0.90)\n",
    "\n",
    "#For in-class activity:\n",
    "#embedding = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed version of the dataset (reduced dimensions)\n",
    "X_low = embedding.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly how many principal components were used to explain 85% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.n_components_, embedding.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great – 90% of the variance is explained by 87 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this perform? Let's visualize that by seeing how well the embedding performs. It takes points in 28x28 images (or 784 pixel values) and approximates that with `n_components` principal components that are each 28x28 (or 784 images).\n",
    "\n",
    "A reconstruction of the original 28x28 image can be found from only the `n_components` 28x28 principal components. How well does that reconstruction look? Let's use the `inverse_tranform` method to reconstruct the digits from the PCA approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use inverse transform to construct approximation to original dataset\n",
    "X_inv = embedding.inverse_transform(X_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the original images and the reconstructed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\n",
    "\n",
    "for left, right in axs:\n",
    "    \n",
    "    # pick an image at random\n",
    "    k = np.random.randint(len(labels))\n",
    "    \n",
    "    # reshape images\n",
    "    img = X[k].reshape(28, 28)\n",
    "    img_est = X_inv[k].reshape(28, 28).clip(min=0,max=255)\n",
    "    \n",
    "    left.imshow(img, cmap='gray_r')\n",
    "    left.set_title('original')\n",
    "    left.axis('off')\n",
    "    right.imshow(img_est, cmap='gray_r')\n",
    "    right.set_title('reconstructed')\n",
    "    right.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful as a compression technique. PCA has some internal structure that allows it to map from `n_components` features to 784 features. Given the quality of the reconstruction, that means the images could be stored with 87 dimensions instead of 784 dimensions (alongside the `n_components` principal components given by the `PCA` object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the principal components of these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = min(embedding.n_components_, 10)\n",
    "principal_components = [embedding.components_[k].reshape(28,28) for k in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=10, figsize=(20, 2))\n",
    "i = 0\n",
    "for img, ax in zip(principal_components, axs):\n",
    "    ax.imshow(img, cmap='bwr')\n",
    "    i = i+1\n",
    "    ax.set_title('PC ' + str(i))\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a closer look: NOTE! these are normalized vectors!\n",
    "plt.imshow(principal_components[0], cmap='bwr')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image is the first principal component. It's the most important because it's the first one. It says \"the most basic approximation using PCA to any digit is somewhat 0 shaped\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the two dimensions, the two dimensions marked as \"most important\" by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = pd.DataFrame(X_low)\n",
    "show.plot.scatter(x=0, y=1, style=\"o\", c=labels, cmap=\"tab10\", s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize only two digits projected onto these two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (labels == 0) | (labels == 1)\n",
    "show[idx].plot.scatter(x=0, y=1, style=\"o\", c=labels[idx], cmap=\"tab10\", s=2, colorbar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
