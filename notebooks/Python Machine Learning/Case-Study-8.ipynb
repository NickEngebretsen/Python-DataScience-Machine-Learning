{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "## ECE 204 Data Science & Engineering\n",
    "\n",
    "Earlier, we used regression to find an intercept and slope to predict a label $y$ from a feature $x$. The equation was:\n",
    "\n",
    "$$y \\approx \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "Then, we discussed multiple regression: finding an intercept and several slopes to predict a label $y$ from features $\\{x,w,z,\\dots\\}$. The equation was:\n",
    "\n",
    "$$y \\approx \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 w + \\cdots$$\n",
    "\n",
    "A related problem is using **one feature** to fit a **polynomial** instead of a linear function. The equation will look like:\n",
    "\n",
    "$$ y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots + \\beta_d x^d$$\n",
    "\n",
    "If our data points look like $(x_i,y_i)$ for $i=1,\\dots,N$, then we can write this as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\y_N\\end{bmatrix} \\approx\n",
    "\\begin{bmatrix}1\\\\1\\\\\\vdots\\\\1\\end{bmatrix}\\beta_0\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "x_1^\\phantom{1} & x_1^2 & \\cdots & x_1^d \\\\\n",
    "x_2^\\phantom{1} & x_2^2 & \\cdots & x_2^d \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_N^\\phantom{1} & x_N^2 & \\cdots & x_N^d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\ \\vdots \\\\\\beta_d\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So even though we're fitting a polynomial in one variable, it's no different than multiple regression where the different powers of $x$ are treated as separate features!\n",
    "\n",
    "Let's see this in action. First step is to create data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# function that creates random observations\n",
    "def make_observations(x, sigma=0.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    noise = sigma * np.random.randn(len(x))\n",
    "    y = 1 - np.exp(-x**2) + noise\n",
    "    return y\n",
    "\n",
    "x = np.linspace(-2, 2, num=20)\n",
    "\n",
    "y = make_observations(x)\n",
    "\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal: predict the `y` values accurately given the input `x`.**\n",
    "\n",
    "To fit this polynomial, we will need polynomial features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, max_degree):\n",
    "    return pd.DataFrame( { i: x ** i for i in range(max_degree+1) } )\n",
    "\n",
    "X_train = polynomial_features(x, max_degree=3)\n",
    "y_train = y\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index = X_train[1]\n",
    "ax = X_train.plot(style=\"o-\", title=\"features for different polynomial degrees\");\n",
    "ax.set_xlabel(\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# note: we don't need to use an \"intercept\" term because this is the same thing as using a\n",
    "# polynomial of degree zero, which we have included in our list of features!\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the coefficients\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way of seeing that is by showing the predicted value over the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_plot(x_train, y_train, degree):\n",
    "    X_train = polynomial_features(x_train, max_degree=degree)\n",
    "    \n",
    "    # make a test set (tightly spaced points on the x-axis)\n",
    "    x_test = np.linspace(x_train.min(), x_train.max(), num=200)\n",
    "    X_test = polynomial_features(x_test, max_degree=degree)\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    df = pd.DataFrame({\"x\": x_train, \"y\": y_train})\n",
    "    df_test = pd.DataFrame({\"x\": x_test, \"y_pred\": y_pred})\n",
    "\n",
    "    # plot original data, then plot predictions on test set (entire x-axis)\n",
    "    ax = df.plot(x=\"x\", y=\"y\", style=\"o\")\n",
    "    ax = df_test.plot(x=\"x\", y=\"y_pred\", ax=ax)\n",
    "    ax.legend();\n",
    "    ax.set_title(\"polynomial fit, degree = \" + str(degree))\n",
    "    ax.set_ylim(y.min()-0.1, y.max()+0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_plot(x, y, degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we select the polynomial degree? Re-run the cell above for different values of `degree`.\n",
    "There is a trade-off between training error and test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We will use k-fold cross-validation to select the best degree to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "max_deg = 10\n",
    "y_train = y\n",
    "\n",
    "for d in range(max_deg+1):\n",
    "    X_train = polynomial_features(x, max_degree=d)\n",
    "    model = LinearRegression(fit_intercept=False)    \n",
    "\n",
    "    # MSE averaged over all folds (using 5-fold CV)\n",
    "    model_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_mse = model_scores.mean()\n",
    "    \n",
    "    # MSE on the training data (entire dataset)\n",
    "    y_pred = model.fit(X_train,y_train).predict(X_train)\n",
    "    train_mse = mean_squared_error(y_pred,y_train)\n",
    "    \n",
    "    print(\"degree = \" + str(d) + \". Train MSE = \", train_mse.round(3), \".  CV MSE = \",  cv_mse.round(3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The MSE is the \"mean squared error\", which is RSS/n (the RSS divided by the number of data points).* \n",
    "The train MSE keeps decreasing with degree, but the MSE from using cross-validation is optimized at around degree=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
